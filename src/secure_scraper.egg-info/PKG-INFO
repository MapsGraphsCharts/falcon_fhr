Metadata-Version: 2.4
Name: secure-scraper
Version: 0.1.0
Summary: Playwright-based scraping toolkit with stealth support for secure targets.
Author: Tim Stafford
Requires-Python: >=3.10
Description-Content-Type: text/markdown
Requires-Dist: playwright>=1.45.0
Requires-Dist: playwright-stealth>=1.0.6
Requires-Dist: httpx>=0.26
Requires-Dist: pydantic-settings>=2.4
Requires-Dist: pyotp>=2.9
Requires-Dist: tenacity>=8.2
Provides-Extra: dev
Requires-Dist: pytest>=8.2; extra == "dev"
Requires-Dist: pytest-asyncio>=0.23; extra == "dev"
Requires-Dist: ruff>=0.5; extra == "dev"

# Secure Scraper

A modular Playwright-powered scraper that supports both headed and headless execution against
fingerprint-aware targets. The project is structured to keep authentication, workflows, storage,
and browser orchestration cleanly separated while making it easy to toggle stealth capabilities.

## Key Features
- Async-first Playwright workflows (headed or headless) with context isolation
- Stealth layer built around [`playwright-stealth`](https://github.com/mattwmaster58/playwright_stealth)
- Config management through environment variables with type validation
- Hooks for login/search/download orchestration and JSON persistence
- Extensible task modules and selector registries for complex pages

## Repository Layout
```
.
├── pyproject.toml
├── README.md
├── scripts/
│   ├── bootstrap.py        # Environment bootstrap & browser installs
│   └── run_scraper.py      # CLI entrypoint orchestrating workflows
├── src/
│   └── secure_scraper/
│       ├── auth/           # Login and multi-factor orchestration
│       ├── config/         # Settings & secrets management
│       ├── core/           # Browser factories, stealth binding, logging
│       ├── selectors/      # Centralised locator maps per page/task
│       ├── storage/        # Data writers (JSON, DB, cloud, ...)
│       ├── tasks/          # High-level scraping workflows (search, download)
│       └── utils/          # Shared utilities (throttling, retries, metrics)
├── tests/                  # Async unit/integration tests
└── data/                   # Local persistence (logs, downloads)
```

## Getting Started
1. Create and populate `.env` from `.env.example`.
2. Install project dependencies:
   ```bash
   python -m venv .venv && source .venv/bin/activate
   pip install -e .[dev]
   ```
3. Install Playwright browsers (Chromium/Firefox/WebKit). The helper script also handles Linux deps:
   ```bash
   python scripts/bootstrap.py
   ```
4. Run the placeholder workflow to ensure wiring is intact:
   ```bash
   python scripts/run_scraper.py
   ```

## Next Steps
- Flesh out `auth.login_flow.LoginFlow` with credential sourcing, MFA hooks, and storage-state reuse.
- Populate selectors under `src/secure_scraper/selectors/` with resilient locator strategies.
- Implement `tasks.search` and `tasks.download` with real navigation and extraction logic.
- Extend `storage.json_writer.JsonStore` with streaming writes or cloud transports as needed.

## References
- Playwright Python docs on browser contexts & storage state (`/microsoft/playwright-python`).
- Stealth patterns from `playwright-stealth` (`/mattwmaster58/playwright_stealth`).
- Consider combining with patched Chromium builds when necessary (e.g. Patchright/Botright).
